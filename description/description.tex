\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}

\usepackage{csquotes}
\usepackage{verbatim}

\usepackage{float}

\usepackage[
    backend=biber,
    style=authoryear-icomp,
    natbib=true,
    url=false,
    doi=true,
    eprint=false
]{biblatex}

%\addbibresource{references.bib}

\input{../generated/constants.tex}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[hidelinks=true, bookmarks=true]{hyperref}
\usepackage{geometry}

\geometry{
    a4paper,
    left=20mm,
    right=20mm,
    top=20mm,
    bottom=20mm,
}

\def\documenttitle{Description of Bot Tau}
\def\name{Frans Englich}

\title{\documenttitle}
\date{\today}
\author{\name \\
        \href{mailto:fenglich@fastmail.fm}{fenglich@fastmail.fm}}

\hypersetup{
    pdfsubject = {\documenttitle},
    pdftitle = {\documenttitle},
    pdfauthor = {\name},                                                     
    pdfcreator = {\name},                                                    
    pdfproducer = {\name, using \LaTeX}
}

\newcommand{\figureTau}[1]{
    \begin{figure}[H]
        \begin{center}
            \includegraphics{../generated/#1.png}
        \end{center}
        %\caption{}
    \end{figure}
}

% Fix for \input in tables. https://tex.stackexchange.com/questions/611786/misplaced-noalign-because-input-before-booktabs-rule
\ExplSyntaxOn
\cs_new:Npn \expandableinput #1
  { \use:c { @@input } { \file_full_name:n {#1} } }
\AddToHook{env/tabular/begin}
  { \cs_set_eq:NN \input \expandableinput }
\ExplSyntaxOff

% TODO Add page numbers

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
\pagenumbering{arabic}

This document describes Bot Tau's trading strategy. Notice that this is one strategy, which is part of a portfolio.

\section{Trading Plan}

\begin{table}[H]
\begin{center}
\caption{Specifics of the trading plan.}
    \begin{tabular}{|l|p{4in}|}
        \hline
        Assets              & Currently undecided \\
        \hline
        Overnight?          & We close positions at end of each trading day, because we don't want overnight exposure. \\
        \hline
        Number of trades per day  &  Currently undecided \\
        \hline
        Performance         &   \begin{itemize}
                                    \item Yearly return $>$ ?
                                    \item Sharp Ratio $>$ ?
                                    \item Calmar Ratio $>$  ?
                                \end{itemize} \\
        \hline
        Over-fitting        & How many times can the strategy be adjusted? How many back tests? \\
        \hline
    \end{tabular}
\end{center}
\end{table}

Risk management conditions:

\begin{itemize}
    \item If we have more than 3 losing trades per day, we stop the algorithm
          for the day.
    \item We stop the algorithm after X \% loss in one month.
    \item We stop the algorithm if the drawdown in live trading becomes  times
          higher than the drawdown in incubation.
\end{itemize}

\section{The Dataset}

The dataset stretches from \constantStartdate \  to \constantEnddate.

\section{Strategy}

\begin{itemize}
    \item Entry signal
    \item Exit signal
    \item Position sizing. Is it static or dynamic in some manner? For instance, percentage of current total capital.
\end{itemize}

\section{Features}

Some form of property, typically derived from the OLHCV. An example is
volatility. The features used are as follows.

\figureTau{feature_BollingerBands}

\figureTau{feature_RSI}


\subsection{Volatility Features}

% TODO Correlation matrix specific for volatility features.

\figureTau{pearsonmatrix}

\figureTau{spearmanmatrix}

\subsubsection{PCA - Principal Component Analysis}

PCA creates new variables, such as principal components, that are linear
combinations of the original variables. It's hence a feature extraction
technique. It is a tool against multicollinearity, by creating new, uncorrelated
features when the original features are highly correlated.

Assumption of Linearity: PCA is a linear technique, hence this is an assumption
of the dataset.

One purpose is to reduce the dimensionality of the data; e.g, reduce the number
of features while keeping as much as possible of the original, important data.

Too many dimensions leads to the "Curse of Dimensionality" which has the
problems of overfitting (modelling noise), sparsity (as dimensions increase,
data points becomes far apart making it harder for algorithms to find meaningful
groups), and practical issues like with visualization and computation.

Note: PCA only applies for normal distributed data. PCA only guarantees zero
correlation. PCA doesn't guarantee for instance independence.

Disadvantages are possible information loss, making it hard to interpret, and
require data scaling. Another problem is risk of overfitting: too many
components or too small data set may lead to models that doesn't generalize
well.

According to IBM "PCA also minimizes, or altogether eliminates, common issues
such as multicollinearity and overfitting."

See: 
\begin{itemize}
    \item \url{https://www.ibm.com/think/topics/principal-component-analysis}
    \item \url{https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/}
    \item \url{https://www.linkedin.com/posts/activity-7412820660596547584-Pb_b}
\end{itemize}

\figureTau{PCA}

% TODO We can pull in the entropy graph.

\subsection{Multicollinearity}

See:

\begin{itemize}
    \item \url{https://www.geeksforgeeks.org/python/detecting-multicollinearity-with-vif-python/}
    \item \url{https://en.wikipedia.org/wiki/Variance_inflation_factor}


Interpretation:

\begin{itemize}
    \item Values near 1 mean predictors are independent.
    \item Values between 1 and 5 shows moderate correlation which is sometime acceptable.
    \item Values above 10 signal problematic multicollinearity requiring action.
\end{itemize}

\begin{table}[H]
\begin{center}
\caption{Variance Inflation Factors (VIF).}
    \begin{tabular}{|l|l| }
        \hline
        Feature & VIF \\
        \hline

        \input{../generated/VIFs}

        \hline

    \end{tabular}
\end{center}
\end{table}

\section{Targets}

\section{Model}

\verbatiminput{../generated/ols_conditions.txt}

\section{Back Test}
\subsection{Drawdown}

Maximum drawdown is \constantMaxdrawdown \%. We consider 20\% an acceptable maximum.

\figureTau{drawdown}

\figureTau{drawdown_dist}

\subsection{Returns}

This is the returns of our trading strategy.

\begin{table}[H]
\begin{center}
\caption{Statistics of returns.}
    \begin{tabular}{ |l|p{1in}| }
        \hline
        Mean returns            & \constantRMean \%     \\
        \hline
        Standard deviation (SD) & \constantStd          \\
        \hline
        Sharpe Ratio (SR)       & \constantSharpeRatio  \\
        \hline
        Calmar Ratio (CR)       & \constantCalmarRatio  \\
        \hline
    \end{tabular}
\end{center}
\end{table}

\figureTau{returns}

The cumulative returns are not compounding, while the annualized returns are.
However, we close the position, meaning compounding isn't relevant.

\figureTau{cumulative_returns}

The transaction cost, $C$, is calculated using the formula, where $t$ is the trade amount:

\begin{equation}
\label{eq:NPV_L}
C = \constantTransactionCommission * t + spread/2
\end{equation}

\figureTau{cumulative_returns_except_trans_costs}

\subsection{Further Robustness Tests}

\section{Live Performance}

The plan is to paper trade in a one-month incubation period.

TODO compare return dist to back test return using Kolmogorov statistical test. % p. 311.

\subsection{Performance Report}

(Copy Discord report.)

\subsection{Trading Journal}

No trading have taken place, so nothing here yet.

%\printbibliography
%\appendix

\end{document}
